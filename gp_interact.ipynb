{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Demystifying Gaussian Process Regression\n",
    "\n",
    "#### By Brett Morris, Jessica Birky, Russell Van Linge\n",
    "\n",
    "### In this tutorial\n",
    "\n",
    "The purpose of this tutorial is to introduce the concepts of Gaussian process regression, and give users an interactive environment for experimenting with Gaussian processes. This tutorial is based on [Rasmussen and Williams (2006)](http://www.gaussianprocess.org/gpml/), but is designed to be more readable and to encourage experimentation!\n",
    "\n",
    "\n",
    "### Getting started\n",
    "\n",
    "\n",
    "Let's generate some fake, one dimensional data, which we'll fit with various techniques.\n",
    "\n",
    "First we need to import matplotlib, numpy, (and optionally scipy) for this notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a small dataset of observations $y$, observed at times $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "ndim = 10\n",
    "y = np.random.rand(ndim)[:, np.newaxis]\n",
    "y -= y.mean()\n",
    "x = np.arange(len(y))[:, np.newaxis]\n",
    "yerr = y.std() / 2 * np.ones_like(x)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***\n",
    "\n",
    "# Linear regression\n",
    "\n",
    "Solutions to the [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) estimators $\\hat{\\beta}$ are\n",
    "\n",
    "$$ \\hat{\\beta} = ({\\bf X}^\\top {\\bf N}^{-1} {\\bf X})^{-1} {\\bf X}^\\top {\\bf N}^{-1} y,$$\n",
    "\n",
    "and their uncertainties are given by\n",
    "\n",
    "$$ \\mathrm{cov} = {\\bf X}^\\top {\\bf N}^{-1} {\\bf X}, $$\n",
    "\n",
    "where ${\\bf N}$ is the matrix of uncertainties on measurements $y$ (${\\bf N}$ is diagonal for measurements with independent Gaussian uncertainties, in this case). \n",
    "\n",
    "Let's implement this using numpy! Note: the `@` operator can be used in python>3.5 to multiply matrices together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Append a column of ones next to the `x` values using `np.vander`: \n",
    "X = np.vander(x.ravel(), 2)\n",
    "inv_N = np.linalg.inv(np.identity(len(x)) * yerr**2)\n",
    "\n",
    "# Solve linear regression: \n",
    "betas = np.linalg.inv(X.T @ inv_N @ X) @ X.T @ inv_N @ y\n",
    "cov = np.linalg.inv(X.T @ inv_N @ X)\n",
    "\n",
    "# Compute best fit line: \n",
    "best_fit = X @ betas \n",
    "err = np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot the result and its uncertainty (here we're plotting just the uncertainty in the intercept and ignoring the uncertainty in the slope, for this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.plot(x, best_fit)\n",
    "plt.fill_between(x.ravel(), best_fit.ravel()-err[1], best_fit.ravel()+err[1], alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*** \n",
    "# Gaussian process regression\n",
    "\n",
    "Gaussian process regression, in this example, is a technique for interpolating between observations or predicting the values of observations at a given time. The linear algebra this time looks a little more complicated, but it's a similar process:   \n",
    "\n",
    "We define a \"**kernel function**\"\n",
    "\n",
    "$$ {\\bf K}({\\bf x_1}, {\\bf x_2}) = \\exp\\left(-\\frac{(x_1 - x_2)^2)}{2 \\ell^2}\\right)   $$\n",
    "\n",
    "A **kernel function** describes the correlation between neighboring points. To start, we're using \"squared exponential\" kernel, which equivalent to saying \"the correlation between neighboring points is defined by a Gaussian with one parameter: $\\ell$.\" We call the tunable parameters within the kernel function **hyperparameters**. Don't let the fancy name intimidate you – all hyperparameters do is define the kernel function, which describes how correlated neighboring points are. \n",
    "\n",
    "##### The $\\ell$ hyperparameter \n",
    "The $\\ell$ parameter sets the autocorrelation timescale, in other words, how far two points need to be from each other in $x$ (time) before they are no longer correlated. If $\\ell$ is large, distant points become more correlated, or in other words, the function becomes smoother. If $\\ell$ is small, the function varies more rapidly with $x$. \n",
    "\n",
    "\n",
    "### Predicting observations at arbitrary $x$\n",
    "Let's say that we've taken observations at times ${\\bf x}$ and we'd like to predict value of $y$ at new times ${\\bf x_\\star}$. The predicted mean is (Equation 2.25 of [Rasmussen and Williams (2006)](http://www.gaussianprocess.org/gpml/)) \n",
    "\n",
    "$$ \\mu_* = {\\bf K(x, x_*)}^\\top ({\\bf K(x, x)} + \\sigma_n^2 {\\bf I})^{-1} {\\bf y},  $$\n",
    "\n",
    "and the covariance is (Equation 2.26)\n",
    "\n",
    "$$ \\mathrm{cov} = {\\bf K}({\\bf x_*}, {\\bf x_*})- {\\bf K(x, x_*)}^\\top ({\\bf K(x, x)}+ \\sigma_n^2 {\\bf I})^{-1} {\\bf K(x, x_*)}, $$ \n",
    "\n",
    "and therefore the error on the predictions will be\n",
    "\n",
    "$$ \\mathrm{err} = \\mathrm{diag}\\left( \\sqrt{\\mathrm{cov}} \\right).$$\n",
    "\n",
    "We can implement this in just a few lines of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def square_distance(x1, x2): \n",
    "    \"\"\"\n",
    "    Compute the squared distance between two vectors `x1` and `x2` \n",
    "    \n",
    "    Note that (x1 - x2)^2 = x1^2 + x2^2 - 2 * x1 * x2, so we can use the short syntax below\n",
    "    to avoid loops: \n",
    "    \n",
    "    For reference, see: \n",
    "    https://medium.com/dataholiks-distillery/l2-distance-matrix-vectorization-trick-26aa3247ac6c\n",
    "    \"\"\"\n",
    "    return np.sum(x1**2, axis=1)[:, np.newaxis] + np.sum(x2**2, axis=1) - 2 * x1 @ x2.T\n",
    "\n",
    "def sq_exp_kernel(x1, x2, ell=1): \n",
    "    \"\"\"\n",
    "    Gaussian Kernel function\n",
    "    \"\"\"\n",
    "    sqdist = square_distance(x1, x2)\n",
    "\n",
    "    return np.exp(-0.5 * sqdist / ell**2)\n",
    "\n",
    "def gaussian_process_regression(x, y, yerr, xtest, kernel): \n",
    "    \"\"\"\n",
    "    Gaussian process regression for column vectors `x` and `y` with \n",
    "    uncertainties `yerr`; predict values at `xtest` using `kernel`\n",
    "    \"\"\"\n",
    "    K = kernel(x, x) + np.identity(len(x)) * yerr**2\n",
    "    k_s = kernel(x, xtest)\n",
    "    k_ss = kernel(xtest, xtest)\n",
    "    inv_K = np.linalg.inv(K)\n",
    "    # The `@` operator in python 3 is the matrix multiplication operator\n",
    "    mu = k_s.T @ inv_K @ y\n",
    "    cov = k_ss - k_s.T @ inv_K @ k_s\n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call our Gaussian process regression method on our data, interpolating between the observations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "# Interpolate `N` observations from the minimum to maximum values of `x` \n",
    "xtest = np.linspace(x.min(), x.max(), N)[:, np.newaxis]\n",
    "\n",
    "mu, cov = gaussian_process_regression(x, y, yerr, xtest, sq_exp_kernel)\n",
    "\n",
    "err = np.sqrt(np.diagonal(cov))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.plot(xtest.ravel(), mu.ravel(), label='GP mean')\n",
    "plt.fill_between(xtest.ravel(), mu.ravel()-err, mu.ravel()+err, alpha=0.3, label='GP uncertainty')\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the blue line shows the predicted value of $y$ for values of $x$ where there were no observations – that's why we say Gaussian process regression is a form of interpolation. It's essentially interpolating between points, where the smoothness of the interpolation is set by the kernel function. \n",
    "\n",
    "But Gaussian process regression is more powerful than just predicting the value of $y$ for arbitrary $x$, it also computes the uncertainty of prediction for arbitrary $x$. The blue region in the figure above shows the uncertainty on the predicted value of $y$ at each $x$. Note that the uncertainty is equivalent to the width of the error bars where there are observations, and the uncertainty gets larger in between observations. \n",
    "\n",
    "Gaussian process regression is often referred to as a form of **\"non-parametric\"** parameter estimation, which is a way of saying that you don't define the functional form of the mean, $\\mu$. $\\mu$ is not a polynomial function, it is not a trigonometric function, it is a highly adaptive interpolation function whose functional form is in fact defined by its covariance matrix (${\\bf K}$), rather than being some function $\\mu = f(x)$ (which means typically it's not easy to write the functional form of the GP in terms of $x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Square-Exponential (Gaussian) Kernel\n",
    "\n",
    "Let's fit the data with an interactive kernel, which allows you to vary the $\\ell$ hyperparameter and see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell):\n",
    "    def sq_exp_kernel_interactive(x1, x2=None, ell=ell): \n",
    "        \"\"\"\n",
    "        Interactive Gaussian Kernel function\n",
    "        \"\"\"\n",
    "        if x2 is not None: \n",
    "            sqdist = square_distance(x1, x2)\n",
    "        else: \n",
    "            sqdist = x1**2\n",
    "            \n",
    "        return np.exp(-0.5 * sqdist / ell**2)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(x, y, yerr, xtest, sq_exp_kernel_interactive)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax[0].errorbar(x, y, yerr, fmt='.', color='k')\n",
    "    ax[0].plot(xtest.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(xtest.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='$x$', ylabel='$y$', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(sq_exp_kernel_interactive(x, x))\n",
    "    \n",
    "    ax[2].plot(xtest, sq_exp_kernel_interactive(xtest))\n",
    "    ax[2].set(xlabel='Lags', ylabel='Autocorrelation', title='Autocorrelation Function')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('Vary the hyperparameter \"ell\", which defines the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the left plot shows the mean model (blue) with its uncertainty (blue shaded region), and the data (black circles). \n",
    "\n",
    "The middle plot shows the covariance matrix – note that it's got most of its weight along the diagonal, which is a mathematical representation of saying \"observations close in $x$ to one another are more correlated than observations far apart in $x$\". The brightness of each pixel corresponds to the strength of the correlation between the two inputs $x_1$ and $x_2$. \n",
    "\n",
    "Alternatively, a simpler way to visualize the behavior of your kernel function is to plot the kernel function directly, which is shown on the right. Note how distant points (large lags) have stronger autocorrelation power when `ell` is large.\n",
    "\n",
    "\n",
    "### Exponential-cosine kernel \n",
    "\n",
    "The exponential-cosine kernel is a cosine function multiplied by a decaying exponential envelope. It is good at fitting quasi-periodic signals (like stellar photometry, for example). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell, period):\n",
    "    def exp_cos(x1, x2=None, sigma=1, ell=ell, period=period): \n",
    "        \"\"\"\n",
    "        Exponentially-decaying cosine function\n",
    "        \"\"\"\n",
    "        if x2 is not None:\n",
    "            sqdist = square_distance(x1, x2)\n",
    "        else: \n",
    "            sqdist = x1**2\n",
    "            \n",
    "        return np.exp(-0.5 * sqdist / ell**2) * np.cos(2*np.pi*np.sqrt(sqdist)/period)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(x, y, yerr, xtest, exp_cos)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax[0].errorbar(x, y, yerr, fmt='.', color='k')\n",
    "    ax[0].plot(xtest.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(xtest.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='$x$', ylabel='$y$', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(exp_cos(x, x))\n",
    "    \n",
    "    ax[2].plot(xtest, exp_cos(xtest))\n",
    "    ax[2].set(xlabel='Lags', ylabel='Autocorrelation', title='Autocorrelation Function')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('Vary the hyperparameters \"ell\" and \"period\", which define the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1), period=(1, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easiest to see what this choice of kernel function (exp-cos) choice looks like by watching the autocorrelation plot above as you change the parameters.  \n",
    "\n",
    "##### The Matrix Inversion Bottleneck\n",
    "\n",
    "Now you may be asking yourself, \"wow this is great, I'd like to apply this technique to a huge dataset. How does the GP scale with the size of the dataset?\" The answer is a bit scary – the computation expense in the general case scales as $\\mathcal{O}(N^3)$. The most computationally expensive step in the process here is the matrix inversion. We're using `numpy`'s standard matrix inversion function, which should work on any matrix.\n",
    "\n",
    "If you want a faster implementation of your Gaussian process regression, you can use a package like [`george`](https://github.com/dfm/george) or [`celerite`](https://github.com/dfm/celerite) which implement faster matrix inversions by taking advantage of special properties of the covariance matrices of certain kernel functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: a rotating star\n",
    "\n",
    "Gaussian processes are not just a means of marginalizing over noise in observations, they can also be used to *measure* physical quantities. Say for example that you are measuring the flux $f$ of a rotating star as a function of time $t$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_period = 3 # days\n",
    "\n",
    "np.random.seed(42)\n",
    "npoints = 30\n",
    "t = np.linspace(0, 10, npoints)[:, np.newaxis]\n",
    "f = np.cos(2*np.pi*t/true_period) + 0.5 * np.random.randn(len(t))[:, np.newaxis]\n",
    "ferr = 0.5 * np.ones_like(t)\n",
    "\n",
    "t_test = np.linspace(t.min(), t.max(), 100)[:, np.newaxis]\n",
    "\n",
    "plt.errorbar(t, f, ferr, color='k', fmt='.')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.ylabel('Flux');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of noise in these observations, but there's also a signal – a periodic signal of rotation from the star. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell, period):\n",
    "    def exp_cos(x1, x2=None, sigma=1, ell=ell, period=period): \n",
    "        \"\"\"\n",
    "        Exponentially-decaying cosine function\n",
    "        \"\"\"\n",
    "        if x2 is not None:\n",
    "            sqdist = square_distance(x1, x2)\n",
    "        else: \n",
    "            sqdist = x1**2\n",
    "            \n",
    "        return np.exp(-0.5 * sqdist / ell**2) * np.cos(2*np.pi*np.sqrt(sqdist)/period)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(t, f, ferr, t, exp_cos)\n",
    "    chi2 = np.sum((mu - f)**2/ferr**2)\n",
    "    lnlike = -0.5 * chi2\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(t, f, ferr, t_test, exp_cos)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax[0].errorbar(t, f, ferr, fmt='.', color='k')\n",
    "    ax[0].plot(t_test.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(t_test.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='Time [days]', ylabel='Flux', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(exp_cos(t, t))\n",
    "    \n",
    "    ax[2].plot(xtest, exp_cos(xtest))\n",
    "    ax[2].set(xlabel='Lags', ylabel='Autocorrelation', title='Autocorrelation Function')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    display(\"Log-likelihood: {0}\".format(lnlike))\n",
    "\n",
    "\n",
    "print('Vary the hyperparameters \"ell\", and \"period\" which define the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1), period=(1, 10, 0.5))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now vary the `period` and `ell` parameters using the sliders – what parameters give you the best fit to the data? Do they match the input period (`true_period`)? You can refer to the log-likelihood, which is printed at the bottom of the visualizatiaon. Log-likelihood is just the opposite of $\\chi^2$, you want to *maximize* the log-likelihood to find the best-fit parameters.\n",
    "\n",
    "### Solving for the maximum-likelihood hyperparameters\n",
    "\n",
    "We can use an optimizer to fit for the best period: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def chi2(p):\n",
    "    ell, period = p\n",
    "    \n",
    "    def exp_cos(x1, x2, sigma=1, ell=ell, period=period): \n",
    "        \"\"\"\n",
    "        Exponentially-decaying cosine function\n",
    "        \"\"\"\n",
    "        sqdist = square_distance(x1, x2)\n",
    "\n",
    "        return np.exp(-0.5 * sqdist / ell**2) * np.cos(2*np.pi*np.sqrt(sqdist)/period)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(t, f, ferr, t, exp_cos)\n",
    "    chi2 = np.sum((mu - f)**2/ferr**2)\n",
    "    return chi2\n",
    "\n",
    "init_parameters = [1.5, 3.5]\n",
    "solution = minimize(chi2, init_parameters)\n",
    "best_ell, best_period = solution.x\n",
    "\n",
    "print(\"Maximum-likelihood period: {0:.2f} days\".format(best_period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value above is close to the input period, good job, you've just used a quasi-periodic kernel function to find the period buried in noisy data! \n",
    "\n",
    "\n",
    "### Choosing a Kernel \n",
    "\n",
    "You should try whenever possible to choose a kernel that aligns with the source of your noise. For example, if your noise is smoothly varying, a squared-exponential kernel might do the trick. If your data is more rapidly varying, you may find a better fit with a Matern kernel. If you know that your data have a quasi-periodic trend (like rotating stars), fit with an exponential-cosine kernel. See Chapter 4 of [Rasmussen and Williams](http://www.gaussianprocess.org/gpml/) for the full range of possibilities.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "You must be careful to set a reasonable prior on the timescale parameter of your GP – if you don't, an optimizer will likely tweak your hyperparameters such that the timescale parameter is small, allowing high frequency variations in the time domain which fit every peak and trough in your data. A good place to start is to prevent the timescale parameters from being similar to or smaller than the space between observations. \n",
    "\n",
    "\n",
    "### Closing thoughts\n",
    "\n",
    "* A Gaussian process (GP) is a non-parametric approach to modeling noisy data\n",
    "\n",
    "* GPs are useful when you don't know the explicit functional form of the correlated noise in your data\n",
    "\n",
    "* GPs are useful for measuring physical parameters in certain circumstances (e.g.: fits for a period in noisy quasi-periodic observations)\n",
    "\n",
    "* GPs are slow in general, so you should take advantage of packages like [`celerite`](http://celerite.readthedocs.io) and [`george`](http://george.readthedocs.io) for fast matrix inversion/decomposition tricks to speed things up\n",
    "\n",
    "* This tutorial only covers GPs in one dimension, though they are extensible to N dimensions (see for example: [GPyTorch](https://github.com/cornellius-gp/gpytorch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
