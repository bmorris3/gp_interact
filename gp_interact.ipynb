{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Demystifying Gaussian Process Regression\n",
    "\n",
    "Let's generate some fake, one dimensional data, which we'll fit with various techniques.\n",
    "\n",
    "First we need to import matplotlib and numpy, the only two dependencies for this notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a small dataset of observations $y$, observed at times $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "ndim = 10\n",
    "x = np.arange(ndim)[:, np.newaxis]\n",
    "y = np.random.rand(ndim)[:, None]\n",
    "y -= y.mean()\n",
    "x = np.arange(len(y))[:, None]\n",
    "yerr = y.std() / 2 * np.ones_like(x)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***\n",
    "\n",
    "# Linear regression\n",
    "\n",
    "Solutions to the [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) estimators $\\hat{\\beta}$ are\n",
    "\n",
    "$$ \\hat{\\beta} = ({\\bf X}^\\top {\\bf N}^{-1} {\\bf X})^{-1} {\\bf X}^\\top {\\bf N}^{-1} y,$$\n",
    "\n",
    "and their uncertainties are given by\n",
    "\n",
    "$$ \\mathrm{cov} = {\\bf X}^\\top {\\bf N}^{-1} {\\bf X}, $$\n",
    "\n",
    "where ${\\bf N}$ is the matrix of uncertainties on measurements $y$ (${\\bf N}$ is diagonal for measurements with independent Gaussian uncertainties, in this case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Append a column of ones next to the `x` values using `np.vander`: \n",
    "X = np.vander(x.ravel(), 2)\n",
    "inv_N = np.linalg.inv(np.identity(len(x)) * yerr**2)\n",
    "\n",
    "# Solve linear regression: \n",
    "betas = np.linalg.inv(X.T @ inv_N @ X) @ X.T @ inv_N @ y\n",
    "cov = np.linalg.inv(X.T @ inv_N @ X)\n",
    "\n",
    "# Compute best fit line: \n",
    "best_fit = X @ betas \n",
    "err = np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot the result and its uncertainty (here we're plotting just the uncertainty in the intercept and ignoring the uncertainty in the slope, for this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.plot(x, best_fit)\n",
    "plt.fill_between(x.ravel(), best_fit.ravel()-err[1], best_fit.ravel()+err[1], alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*** \n",
    "# Gaussian process regression\n",
    "\n",
    "Gaussian process regression, in this example, is a technique for interpolating between observations or predicting the values of observations at a given time. The linear algebra this time looks a little more complicated, but it's a similar process:   \n",
    "\n",
    "We define a \"**kernel function**\"\n",
    "\n",
    "$$ {\\bf K}({\\bf x_1}, {\\bf x_2}) = \\sigma^2 \\exp\\left(-\\frac{(x_1 - x_2)^2)}{2 \\ell^2}\\right)   $$\n",
    "\n",
    "A **kernel function** describes the correlation between neighboring points. To start, we're using \"squared exponential\" kernel, which equivalent to saying \"the correlation between neighboring points is defined by a Gaussian with two parameters, $\\sigma$ and $\\ell$.\" We call the tunable parameters within the kernel function **hyperparameters**. Don't let the fancy name intimidate you – all hyperparameters do is define how correlated neighboring points are. \n",
    "\n",
    "### The $\\ell$ hyperparameter \n",
    "The $\\ell$ parameter sets the autocorrelation timescale, in other words, how far two points need to be from each other in $x$ (time) before they are no longer correlated. If $\\ell$ is large, distant points become more correlated, or in other words, the function becomes smoother. If $\\ell$ is small, the function varies more rapidly with $x$. \n",
    "\n",
    "\n",
    "### Gaussian Process Regression\n",
    "From [Rasmussen and Williams (2006)](http://www.gaussianprocess.org/gpml/), let's say that we've taken observations at times $X$ and we'd like to predict value of $y$ at new times $X_\\star$. The predicted mean is (Equation 2.25) \n",
    "\n",
    "$$ \\mu = {\\bf K(x, x_*)}^\\top ({\\bf K(X, X)} + \\sigma_n {\\bf I})^{-1} {\\bf y},  $$\n",
    "\n",
    "and the covariance is (Equation 2.26)\n",
    "\n",
    "$$ \\mathrm{cov} = {\\bf K}({\\bf X_*}, {\\bf X_*})- {\\bf k(X, X_*)}^\\top ({\\bf K(X, X)}+ \\sigma_n {\\bf I})^{-1} {\\bf k_*}, $$ \n",
    "\n",
    "and therefore the error on the predictions will be\n",
    "\n",
    "$$ \\mathrm{err} = \\mathrm{diag}\\left( \\sqrt{\\mathrm{cov}} \\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def square_distance(x1, x2): \n",
    "    \"\"\"\n",
    "    Compute the squared distance between two vectors `x1` and `x2` \n",
    "    \n",
    "    Note that (x1 - x2)^2 = x1^2 + x2^2 - 2 * x1 * x2, so we can use the short syntax below\n",
    "    to avoid loops: \n",
    "    \n",
    "    For reference, see: \n",
    "    https://medium.com/dataholiks-distillery/l2-distance-matrix-vectorization-trick-26aa3247ac6c\n",
    "    \"\"\"\n",
    "    return np.sum(x1**2, axis=1)[:, np.newaxis] + np.sum(x2**2, axis=1) - 2 * np.dot(x1, x2.T)\n",
    "\n",
    "def sq_exp_kernel(x1, x2, sigma=2, ell=1): \n",
    "    \"\"\"\n",
    "    Gaussian Kernel function\n",
    "    \"\"\"\n",
    "    sqdist = square_distance(x1, x2)\n",
    "\n",
    "    return sigma**2 * np.exp(-0.5 * sqdist / ell**2)\n",
    "\n",
    "def gaussian_process_regression(x, y, yerr, xtest, kernel): \n",
    "    \"\"\"\n",
    "    Gaussian process regression for column vectors `x` and `y` with \n",
    "    uncertainties `yerr`; predict values at `xtest` using `kernel`\n",
    "    \"\"\"\n",
    "    K = kernel(x, x) + np.identity(len(x)) * yerr**2\n",
    "    k_s = kernel(x, xtest)\n",
    "    k_ss = kernel(xtest, xtest)\n",
    "    inv_K = np.linalg.inv(K)\n",
    "    # The `@` operator in python 3 is the matrix multiplication operator\n",
    "    mu = k_s.T @ inv_K @ y\n",
    "    cov = k_ss - k_s.T @ inv_K @ k_s\n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "xtest = np.linspace(x.min(), x.max(), N)[:, np.newaxis]\n",
    "\n",
    "mu, cov = gaussian_process_regression(x, y, yerr, xtest, sq_exp_kernel)\n",
    "\n",
    "err = np.sqrt(np.diagonal(cov))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.plot(xtest.ravel(), mu.ravel(), label='GP mean')\n",
    "plt.fill_between(xtest.ravel(), mu.ravel()-err, mu.ravel()+err, alpha=0.3, label='GP uncertainty')\n",
    "plt.legend(loc='upper center')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the blue line shows the predicted value of $y$ for values of $x$ where there were no observations – that's why we say Gaussian process regression is a form of interpolation. It's essentially interpolating between points, where the smoothness of the interpolation is set by the kernel function. \n",
    "\n",
    "But Gaussian process regression is more powerful than just predicting the value of $y$ for arbitrary $x$, it also computes the uncertainty of prediction at for arbitrary $x$. The blue region in the figure above shows the uncertainty on the predicted value of $y$ at each $x$. Note that the uncertainty is equivalent to the width of the error bars where there are observations, and the uncertainty gets larger in between observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Square-Exponential (Gaussian) Kernel\n",
    "\n",
    "which allows you to vary the $\\ell$ hyperparameter and see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell):\n",
    "    def sq_exp_kernel_interactive(x1, x2, sigma=1, ell=ell): \n",
    "        \"\"\"\n",
    "        Interactive Gaussian Kernel function\n",
    "        \"\"\"\n",
    "        sqdist = square_distance(x1, x2)\n",
    "\n",
    "        return sigma**2 * np.exp(-0.5 * sqdist / ell**2)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(x, y, yerr, xtest, sq_exp_kernel_interactive)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].errorbar(x, y, yerr, fmt='.', color='k')\n",
    "    ax[0].plot(xtest.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(xtest.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='$x$', ylabel='$y$', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(sq_exp_kernel_interactive(x, x))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print('Vary the hyperparameter \"ell\", which defines the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential-cosine kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell, period):\n",
    "    def exp_cos(x1, x2, sigma=1, ell=ell, period=period): \n",
    "        \"\"\"\n",
    "        Interactive Gaussian Kernel function\n",
    "        \"\"\"\n",
    "        sqdist = square_distance(x1, x2)\n",
    "\n",
    "        return sigma**2 * np.exp(-0.5 * sqdist / ell**2) * np.cos(2*np.pi*np.sqrt(sqdist)/period)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(x, y, yerr, xtest, exp_cos)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].errorbar(x, y, yerr, fmt='.', color='k')\n",
    "    ax[0].plot(xtest.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(xtest.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='$x$', ylabel='$y$', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(exp_cos(x, x))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print('Vary the hyperparameters \"ell\" and \"period\", which define the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1), period=(1, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
