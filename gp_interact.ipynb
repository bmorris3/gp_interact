{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Demystifying Gaussian Process Regression\n",
    "\n",
    "#### By Brett Morris with Jessica Birky, Russell Van Linge\n",
    "\n",
    "### In this tutorial\n",
    "\n",
    "The purpose of this tutorial is to introduce the concepts of Gaussian process regression, and give users an interactive environment for experimenting with Gaussian processes. This tutorial is based on [Rasmussen and Williams (2006)](http://www.gaussianprocess.org/gpml/), but is designed to be more readable and to encourage experimentation!\n",
    "\n",
    "\n",
    "### Getting started\n",
    "\n",
    "\n",
    "Let's generate some fake, one dimensional data, which we'll fit with various techniques.\n",
    "\n",
    "First we need to import matplotlib, numpy, scipy for this notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive\n",
    "import ipywidgets as widgets\n",
    "from scipy.optimize import minimize\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's generate a small dataset of observations $y$, observed at times $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "ndim = 10\n",
    "y = np.random.rand(ndim)[:, np.newaxis]\n",
    "y -= y.mean()\n",
    "x = np.arange(len(y))[:, np.newaxis]\n",
    "yerr = (y.std() + 0.1*np.random.randn(len(x))) / 2\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***\n",
    "\n",
    "# Linear regression\n",
    "\n",
    "Solutions to the [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) estimators $\\hat{\\beta}$ are\n",
    "\n",
    "$$ \\hat{\\beta} = ({\\bf X}^\\top {\\bf N}^{-1} {\\bf X})^{-1} {\\bf X}^\\top {\\bf N}^{-1} y,$$\n",
    "\n",
    "and their uncertainties are given by\n",
    "\n",
    "$$ \\mathrm{cov} = {\\bf X}^\\top {\\bf N}^{-1} {\\bf X}, $$\n",
    "\n",
    "where ${\\bf N}$ is the matrix of uncertainties on measurements $y$ (${\\bf N}$ is diagonal for measurements with independent Gaussian uncertainties, in this case). \n",
    "\n",
    "Let's implement this using numpy! Note: the `@` operator can be used in python>3.5 to multiply matrices together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Append a column of ones next to the `x` values using `np.vander`: \n",
    "X = np.vander(x.ravel(), 2)\n",
    "inv_N = np.linalg.inv(np.identity(len(x)) * yerr**2)\n",
    "\n",
    "# Solve linear regression: \n",
    "betas = np.linalg.inv(X.T @ inv_N @ X) @ X.T @ inv_N @ y\n",
    "cov = np.linalg.inv(X.T @ inv_N @ X)\n",
    "\n",
    "# Compute best fit line: \n",
    "best_fit = X @ betas \n",
    "err = np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot the result and its uncertainty (here we're plotting just the uncertainty in the intercept and ignoring the uncertainty in the slope, for this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr, fmt='.', color='k')\n",
    "plt.plot(x, best_fit)\n",
    "plt.fill_between(x.ravel(), best_fit.ravel()-err[1], best_fit.ravel()+err[1], alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*** \n",
    "# Gaussian process regression\n",
    "\n",
    "Gaussian process regression, in this example, is a technique for interpolating between observations or predicting the values of observations at a given time. The linear algebra this time looks a little more complicated, but it's a similar process:   \n",
    "\n",
    "We define a \"**kernel function**\"\n",
    "\n",
    "$$ {\\bf K}({\\bf x_1}, {\\bf x_2}) = \\exp\\left(-\\frac{(x_1 - x_2)^2)}{2 \\ell^2}\\right)   $$\n",
    "\n",
    "A **kernel function** describes the correlation between neighboring points. To start, we're using \"squared exponential\" kernel, which equivalent to saying \"the correlation between neighboring points is defined by a Gaussian with one parameter: $\\ell$.\" We call the tunable parameters within the kernel function **hyperparameters**. Don't let the fancy name intimidate you – all hyperparameters do is define the kernel function, which describes how correlated neighboring points are. \n",
    "\n",
    "##### The $\\ell$ hyperparameter \n",
    "The $\\ell$ parameter sets the autocorrelation timescale, in other words, how far two points need to be from each other in $x$ (time) before they are no longer correlated. If $\\ell$ is large, distant points become more correlated, or in other words, the function becomes smoother. If $\\ell$ is small, the function varies more rapidly with $x$. \n",
    "\n",
    "\n",
    "### Predicting observations at arbitrary $x$\n",
    "Let's say that we've taken observations at times ${\\bf x}$ and we'd like to predict value of $y$ at new times ${\\bf x_\\star}$. The predicted mean is (Equation 2.25 of [Rasmussen and Williams (2006)](http://www.gaussianprocess.org/gpml/)) \n",
    "\n",
    "$$ \\mu_* = {\\bf K(x, x_*)}^\\top ({\\bf K(x, x)} + \\sigma_n^2 {\\bf I})^{-1} {\\bf y},  $$\n",
    "\n",
    "and the covariance is (Equation 2.26)\n",
    "\n",
    "$$ \\mathrm{cov} = {\\bf K}({\\bf x_*}, {\\bf x_*})- {\\bf K(x, x_*)}^\\top ({\\bf K(x, x)}+ \\sigma_n^2 {\\bf I})^{-1} {\\bf K(x, x_*)}, $$ \n",
    "\n",
    "and therefore the error on the predictions will be\n",
    "\n",
    "$$ \\mathrm{err} = \\mathrm{diag}\\left( \\sqrt{\\mathrm{cov}} \\right).$$\n",
    "\n",
    "We can implement this in just a few lines of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def square_distance(x1, x2): \n",
    "    \"\"\"\n",
    "    Compute the squared distance between two vectors `x1` and `x2` \n",
    "    \n",
    "    Note that (x1 - x2)^2 = x1^2 + x2^2 - 2 * x1 * x2, so we can use the short syntax below\n",
    "    to avoid loops: \n",
    "    \n",
    "    For reference, see: \n",
    "    https://medium.com/dataholiks-distillery/l2-distance-matrix-vectorization-trick-26aa3247ac6c\n",
    "    \"\"\"\n",
    "    return np.sum(x1**2, axis=1)[:, np.newaxis] + np.sum(x2**2, axis=1) - 2 * x1 @ x2.T\n",
    "\n",
    "def sq_exp_kernel(x1, x2, ell=1): \n",
    "    \"\"\"\n",
    "    Gaussian Kernel function\n",
    "    \"\"\"\n",
    "    sqdist = square_distance(x1, x2)\n",
    "\n",
    "    return np.exp(-0.5 * sqdist / ell**2)\n",
    "\n",
    "def gaussian_process_regression(x, y, yerr, xtest, kernel, **kwargs): \n",
    "    \"\"\"\n",
    "    Gaussian process regression for column vectors `x` and `y` with \n",
    "    uncertainties `yerr`; predict values at `xtest` using `kernel`\n",
    "    \"\"\"\n",
    "    K = kernel(x, x, **kwargs) + np.identity(len(x)) * yerr**2\n",
    "    k_s = kernel(x, xtest, **kwargs)\n",
    "    k_ss = kernel(xtest, xtest, **kwargs)\n",
    "    inv_K = np.linalg.inv(K)\n",
    "    # The `@` operator in python 3 is the matrix multiplication operator\n",
    "    mu = k_s.T @ inv_K @ y\n",
    "    cov = k_ss - k_s.T @ inv_K @ k_s\n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's call our Gaussian process regression method on our data, interpolating between the observations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "# Interpolate `N` observations from the minimum to maximum values of `x` \n",
    "xtest = np.linspace(x.min(), x.max(), N)[:, np.newaxis]\n",
    "\n",
    "def gp_interact(error_scale):\n",
    "    mu, cov = gaussian_process_regression(x, y, yerr*error_scale, xtest, sq_exp_kernel)\n",
    "\n",
    "    err = np.sqrt(np.diagonal(cov))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.errorbar(x, y, yerr*error_scale, fmt='.', color='k')\n",
    "    plt.plot(xtest.ravel(), mu.ravel(), label='GP mean')\n",
    "    plt.fill_between(xtest.ravel(), mu.ravel()-err, mu.ravel()+err, alpha=0.3, label='GP uncertainty')\n",
    "    plt.legend(loc='upper center')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y');\n",
    "    \n",
    "print('Vary the uncertainty with the scaling parameter \"error_scale\", gets multplied by the error in y:')\n",
    "interactive_plot = interactive(gp_interact, error_scale=(0.1, 1, 0.1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the figure above, the blue line shows the predicted value of $y$ for values of $x$ where there were no observations – that's why we say Gaussian process regression is a form of interpolation. It's essentially interpolating between points, where the smoothness of the interpolation is set by the kernel function. \n",
    "\n",
    "But Gaussian process regression is more powerful than just predicting the value of $y$ for arbitrary $x$, it also computes the *uncertainty* of the prediction for arbitrary $x$. The blue region in the figure above shows the uncertainty on the predicted value of $y$ at each $x$. Note that the uncertainty is roughly equivalent to the width of the error bars where there are observations, and the uncertainty gets larger in between observations. You can change the uncertainties on each point and see its effect on the uncertainty in the predicted mean by sliding the `error_scale` slider above the plot. As your observations shift towards higher precision (smaller uncetainties), so does the Gaussian process model.  \n",
    "\n",
    "Gaussian process regression is often referred to as a form of **\"non-parametric\"** parameter estimation, which is a way of saying that you don't define the functional form of the mean, $\\mu$. $\\mu$ is not a polynomial function, it is not a trigonometric function, it is a highly adaptive interpolation function whose functional form is in fact defined by its covariance matrix (${\\bf K}$), rather than being some function $\\mu = f(x)$ (which means typically it's not easy to write the functional form of the GP in terms of $x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Square-Exponential (Gaussian) Kernel\n",
    "\n",
    "Let's fit the data with an interactive kernel, which allows you to vary the $\\ell$ hyperparameter and see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell):\n",
    "    def sq_exp_kernel_interactive(x1, x2=None, ell=ell): \n",
    "        \"\"\"\n",
    "        Interactive Gaussian Kernel function\n",
    "        \"\"\"\n",
    "        if x2 is not None: \n",
    "            sqdist = square_distance(x1, x2)\n",
    "        else: \n",
    "            sqdist = x1**2\n",
    "            \n",
    "        return np.exp(-0.5 * sqdist / ell**2)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(x, y, yerr, xtest, sq_exp_kernel_interactive)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax[0].errorbar(x, y, yerr, fmt='.', color='k')\n",
    "    ax[0].plot(xtest.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(xtest.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='$x$', ylabel='$y$', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(sq_exp_kernel_interactive(x, x))\n",
    "    \n",
    "    ax[2].plot(xtest, sq_exp_kernel_interactive(xtest))\n",
    "    ax[2].set(xlabel='Lags', ylabel='Autocorrelation', title='Autocorrelation Function')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('Vary the hyperparameter \"ell\", which defines the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the figure above, the left plot shows the mean model (blue) with its uncertainty (blue shaded region), and the data (black circles). \n",
    "\n",
    "The middle plot shows the covariance matrix – note that it's got most of its weight along the diagonal, which is a mathematical representation of saying \"observations close in $x$ to one another are more correlated than observations far apart in $x$\". The brightness of each pixel corresponds to the strength of the correlation between the two inputs $x_1$ and $x_2$. \n",
    "\n",
    "Alternatively, a simpler way to visualize the behavior of your kernel function is to plot the kernel function directly, which is shown on the right. Note how distant points (large lags) have stronger autocorrelation power when `ell` is large.\n",
    "\n",
    "\n",
    "### Exponential-cosine kernel \n",
    "\n",
    "The exponential-cosine kernel is a cosine function multiplied by a decaying exponential envelope. It is good at fitting quasi-periodic signals (like stellar photometry, for example). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell, period):\n",
    "    def exp_cos(x1, x2=None, sigma=1, ell=ell, period=period): \n",
    "        \"\"\"\n",
    "        Exponentially-decaying cosine function\n",
    "        \"\"\"\n",
    "        if x2 is not None:\n",
    "            sqdist = square_distance(x1, x2)\n",
    "        else: \n",
    "            sqdist = x1**2\n",
    "            \n",
    "        return np.exp(-0.5 * sqdist / ell**2) * np.cos(2*np.pi*np.sqrt(sqdist)/period)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(x, y, yerr, xtest, exp_cos)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax[0].errorbar(x, y, yerr, fmt='.', color='k')\n",
    "    ax[0].plot(xtest.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(xtest.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='$x$', ylabel='$y$', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(exp_cos(x, x))\n",
    "    \n",
    "    ax[2].plot(xtest, exp_cos(xtest))\n",
    "    ax[2].set(xlabel='Lags', ylabel='Autocorrelation', title='Autocorrelation Function')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('Vary the hyperparameters \"ell\" and \"period\", which define the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1), period=(1, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's easiest to see what this choice of kernel function (exp-cos) choice looks like by watching the autocorrelation plot above as you change the parameters.  \n",
    "\n",
    "##### The Matrix Inversion Bottleneck\n",
    "\n",
    "Now you may be asking yourself, \"wow this is great, I'd like to apply this technique to a huge dataset. How does the GP scale with the size of the dataset?\" The answer is a bit scary – the computation expense in the general case scales as $\\mathcal{O}(N^3)$. The most computationally expensive step in the process here is the matrix inversion. We're using `numpy`'s standard matrix inversion function, which should work on any matrix.\n",
    "\n",
    "If you want a faster implementation of your Gaussian process regression, you can use a package like [`george`](https://github.com/dfm/george) or [`celerite`](https://github.com/dfm/celerite) which implement faster matrix inversions by taking advantage of special properties of the covariance matrices of certain kernel functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*** \n",
    "\n",
    "# Example: a rotating star\n",
    "\n",
    "Gaussian processes are not just a means of marginalizing over noise in observations, they can also be used to *measure* physical quantities. Say for example that you are measuring the flux $f$ of a rotating star as a function of time $t$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "true_period = 3.5 # days\n",
    "\n",
    "np.random.seed(42)\n",
    "npoints = 50\n",
    "t = np.linspace(0, 2*true_period, npoints)[:, np.newaxis]\n",
    "\n",
    "# Generate a sinusoidal signal\n",
    "sinusoid = np.sin(2*np.pi*t/true_period) \n",
    "\n",
    "# Assing non-uniform uncertainties to each point\n",
    "ferr = 0.2 + 0.05 * np.random.randn(len(t))\n",
    "\n",
    "# Add correlated noise to the sinusoidal signal\n",
    "cov = np.diag((2*ferr)**2) + 0.8 * np.exp(-0.5 * square_distance(t, t) / 2)\n",
    "f = np.random.multivariate_normal(sinusoid.ravel(), cov)[:, np.newaxis]\n",
    "\n",
    "t_test = np.linspace(t.min(), t.max() + 5, 100)[:, np.newaxis]\n",
    "\n",
    "plt.errorbar(t, f, ferr, color='k', fmt='.')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.ylabel('Flux');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There's a lot of noise in these observations, but there's also a signal – a periodic signal of rotation from the star. Let's fit these data with an exoponentially-decaying cosine kernel, and extrapolate our results into _the future_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_interact(ell, period):\n",
    "    def exp_cos(x1, x2=None, sigma=1, ell=ell, period=period): \n",
    "        \"\"\"\n",
    "        Exponentially-decaying cosine function\n",
    "        \"\"\"\n",
    "        if x2 is not None:\n",
    "            sqdist = square_distance(x1, x2)\n",
    "        else: \n",
    "            sqdist = x1**2\n",
    "            \n",
    "        return np.exp(-0.5 * sqdist / ell**2) * np.cos(2*np.pi*np.sqrt(sqdist)/period)\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(t, f, ferr, t, exp_cos)\n",
    "    chi2 = np.sum((mu - f)**2/ferr**2)\n",
    "    lnlike = -0.5 * chi2\n",
    "    \n",
    "    mu, cov = gaussian_process_regression(t, f, ferr, t_test, exp_cos)\n",
    "    err = np.sqrt(np.diag(cov))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    ax[0].errorbar(t, f, ferr, fmt='.', color='k')\n",
    "    ax[0].plot(t_test.ravel(), mu.ravel(), label='GP Mean')\n",
    "    ax[0].fill_between(t_test.ravel(),  mu.ravel()-err, mu.ravel()+err, alpha=0.2, label='GP Uncertainty')\n",
    "    ax[0].set(xlabel='Time [days]', ylabel='Flux', title='GP Regression')\n",
    "    ax[0].legend(loc='upper center')\n",
    "    \n",
    "    ax[1].set(xlabel='$x_1$', ylabel='$x_2$', title='Covariance matrix')\n",
    "    ax[1].imshow(exp_cos(t, t))\n",
    "    \n",
    "    ax[2].plot(xtest, exp_cos(xtest))\n",
    "    ax[2].set(xlabel='Lags', ylabel='Autocorrelation', title='Autocorrelation Function')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    display(\"Log-likelihood: {0}\".format(lnlike))\n",
    "\n",
    "\n",
    "print('Vary the hyperparameters \"ell\", and \"period\" which define the autocorrelation timescale:')\n",
    "interactive_plot = interactive(gp_interact, ell=(1, 10, 1), period=(1, 10, 0.5))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now vary the `period` and `ell` parameters using the sliders – what parameters give you the best fit to the data? Do they match the input period (`true_period`)? You can refer to the log-likelihood, which is printed at the bottom of the visualizatiaon. Log-likelihood is just the opposite of $\\chi^2$, you want to *maximize* the log-likelihood to find the best-fit parameters.\n",
    "\n",
    "### Solving for the maximum-likelihood hyperparameters\n",
    "\n",
    "We can use an optimizer to fit for the best period: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def chi2(p):\n",
    "    ell, period = p\n",
    "    \n",
    "    def exp_cos(x1, x2, sigma=1, ell=ell, period=period): \n",
    "        \"\"\"\n",
    "        Exponentially-decaying cosine function\n",
    "        \"\"\"\n",
    "        sqdist = square_distance(x1, x2)\n",
    "\n",
    "        return np.exp(-0.5 * sqdist / ell**2) * np.cos(2*np.pi*np.sqrt(sqdist)/period)\n",
    "    \n",
    "    # Note: x and xtest are the same here since we want to evaluate the\n",
    "    # Gaussian process at the same x values as our observations\n",
    "    mu, cov = gaussian_process_regression(t, f, ferr, t, exp_cos)\n",
    "    chi2 = np.sum((mu - f)**2/ferr**2)\n",
    "    return chi2\n",
    "\n",
    "# Initial guesses for [ell, period]: \n",
    "init_parameters = [2, 3.5]\n",
    "\n",
    "# Place uniform priors (boundaries) on the values of [ell, period]\n",
    "# that are explored by the minimizer: \n",
    "bounds = [[2, 10], [2, 10]]\n",
    "\n",
    "# Use `minimize` to minimize the chi^2 using the L-BFGS-B method\n",
    "solution = minimize(chi2, init_parameters, method='L-BFGS-B', bounds=bounds)\n",
    "best_ell, best_period = solution.x\n",
    "\n",
    "# Print the results: \n",
    "print(\"Maximum-likelihood ell: {0:.2f} days\".format(best_ell))\n",
    "print(\"Maximum-likelihood period: {0:.2f} days\".format(best_period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value above is close to the input period, good job, you've just used a quasi-periodic kernel function to find the period lurking in noisy data! \n",
    "\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "You must be careful to set a reasonable prior (especially a lower limit) on the timescale parameter `ell` for your GP – if you don't, an optimizer will likely tweak your hyperparameters such that the timescale parameter `ell` is small, allowing high frequency variations in the time domain which fit every peak and trough in your data. \n",
    "\n",
    "A good place to start is to prevent the timescale parameters from being similar to or smaller than the space between observations. That's what we did in the example above: notice the `bounds` parameter that we gave to the `minimize` function: we only allowed the timescale parameter `ell` to be twice the separation between points or greater. The best-fit result has a timescale parameter that is equivalent to the lower limit, signaling to us that it was important that we put that limit in place.\n",
    "\n",
    "\n",
    "### Choosing a Kernel \n",
    "\n",
    "You should try whenever possible to choose a kernel that aligns with the source of your noise. For example, if your noise is smoothly varying, a squared-exponential kernel might do the trick. If your data is more rapidly varying, you may find a better fit with a Matern kernel. If you know that your data have a quasi-periodic trend (like rotating stars), fit with an exponential-cosine kernel. See Chapter 4 of [Rasmussen and Williams](http://www.gaussianprocess.org/gpml/) for the full range of possibilities.\n",
    "\n",
    "*** \n",
    "\n",
    "# Fitting a sinusoidal model to the data with a Gaussian process\n",
    "\n",
    "Now you may be asking, \"why bother fitting a Gaussian process to the observations above, when we know the signal is sinusoidal\"? If you know the functional form of the signal you're trying to fit, then you should use it! In Gaussian process terminology we'd call that \"fitting for the **mean model**\", in other words, the model which you can subtract from your data in order to create a time series of observations with zero mean is the mean model.\n",
    "\n",
    "Let's fit a sinusoidal model to the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sine_model(p):\n",
    "    amplitude, period = p\n",
    "    return amplitude * np.sin(2*np.pi*t/period)\n",
    "\n",
    "def chi2(p):\n",
    "    return np.sum((sine_model(p) - f)**2/ferr**2)\n",
    "\n",
    "# Initial guesses for [amplitude, period]: \n",
    "init_parameters = [1.1, 3.2]\n",
    "\n",
    "# Place uniform priors (boundaries) on the values of [amp, period]\n",
    "# that are explored by the minimizer: \n",
    "bounds = [[0.5, 2], [2, 10]]\n",
    "\n",
    "# Use `minimize` to minimize the chi^2 using the L-BFGS-B method\n",
    "solution = minimize(chi2, init_parameters, method='L-BFGS-B', bounds=bounds)\n",
    "best_amp, best_period = solution.x\n",
    "\n",
    "# Print the results: \n",
    "print(\"Maximum-likelihood period: {0:.2f} days\".format(best_period))\n",
    "\n",
    "# Plot the result: \n",
    "plt.errorbar(t, f, ferr, color='k', fmt='.')\n",
    "plt.plot(t, sine_model(solution.x))\n",
    "plt.xlabel('Time [days]')\n",
    "plt.ylabel('Flux');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimizer is finding a best-fit solution that's close to the true solution, but the fit isn't so great. Let's plot the residuals (`data - model`) with reasonable solutions for the amplitude and period of the sinusoidal mean model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(t, f - sine_model([1, true_period]), ferr, color='k', fmt='.')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.ylabel('Residuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the residuals above do not look like white noise – they're **correlated** in time. This is exactly **when to use a Gaussian process**: when you want to fit a model but there is correlated noise in your data. \n",
    "\n",
    "Now let's fit the model simultaneously a Gaussian process with a squared-exponential to pick up that correlated noise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sine_model_gp(p):\n",
    "    amplitude, period, ell = p\n",
    "    return amplitude * np.sin(2*np.pi*t/period)\n",
    "\n",
    "\n",
    "def gp_model(p):\n",
    "    amplitude, period, ell = p\n",
    "    mean_model = sine_model_gp(p)\n",
    "    mu, cov = gaussian_process_regression(t, f - mean_model, ferr, t, \n",
    "                                          sq_exp_kernel, ell=ell)\n",
    "    residuals = f - mean_model - mu\n",
    "    return mu, cov, residuals\n",
    "\n",
    "def chi2(p):\n",
    "    mu, cov, residuals = gp_model(p)\n",
    "    return np.sum(residuals**2/ferr**2)\n",
    "\n",
    "# Initial guesses for [amplitude, period]: \n",
    "init_parameters = [1.1, 3.5, 2]\n",
    "\n",
    "# Place uniform priors (boundaries) on the values of [amp, period]\n",
    "# that are explored by the minimizer: \n",
    "bounds = [[0.5, 2], [2, 10], [2, 10]]\n",
    "\n",
    "# Use `minimize` to minimize the chi^2 using the L-BFGS-B method\n",
    "solution = minimize(chi2, init_parameters, method='L-BFGS-B', bounds=bounds)\n",
    "best_amp, best_period, best_ell = solution.x\n",
    "\n",
    "# Print the results: \n",
    "print(\"Maximum-likelihood period: {0:.2f} days\".format(best_period))\n",
    "print(\"Maximum-likelihood ell: {0:.2f} days\".format(best_ell))\n",
    "\n",
    "# Plot the result: \n",
    "mu, cov, residuals = gp_model(solution.x)\n",
    "sigma = np.sqrt(np.diag(cov))\n",
    "\n",
    "# First plot the data and the mean model\n",
    "fig, ax = plt.subplots(3, 1, figsize=(6, 10), sharex=True)\n",
    "ax[0].errorbar(t, f, ferr, color='k', fmt='.')\n",
    "ax[0].plot(t, sine_model_gp(solution.x), label='mean model solution')\n",
    "ax[0].set(ylabel='Flux')\n",
    "ax[0].legend()\n",
    "\n",
    "# Next plot the data minus the mean model, and the best-fit GP\n",
    "ax[1].errorbar(t, f - sine_model_gp(solution.x), ferr, color='k', fmt='.')\n",
    "ax[1].plot(t, mu, color='C1', label='GP mean')\n",
    "ax[1].fill_between(t.ravel(), mu.ravel() - sigma.ravel(), \n",
    "                   mu.ravel() + sigma.ravel(), \n",
    "                   color='C1', label='GP uncertainty', alpha=0.3)\n",
    "ax[1].set(ylabel='Residuals')\n",
    "ax[1].legend()\n",
    "\n",
    "# Lastly, plot the residuals after the GP and mean model are removed\n",
    "ax[2].errorbar(t, residuals, ferr, color='k', fmt='.')\n",
    "ax[2].set(xlabel='Time [days]', ylabel='Residuals without correlated noise')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we are fitting for the Gaussian process hyperparameter `ell` at the same time as we fit for the sinusoidal model parameters: amplitude and period. The periodic component is being fit by the periodic mean function that we chose (a sine function), and the non-periodic, correlated noise present in our data was fit by the Gaussian process using a smooth squared-exponential kernel. \n",
    "\n",
    "The uppermost plot shows the best-fit sinusoidal model (blue) which gets close the correct period. It's clear though that the fit to the data isn't very good. The middle plot shows the observations subtracted by the best-fit mean model, which clearly show the correlated noise we discovered earlier. The middle plot also shows the Gaussian process model (orange), which smoothly soaks up the correlated noise. The bottom plot shows the residuals after the mean model and the Gaussian process model have been subtracted off, leaving you with less correlated, nearly \"white\" noise.\n",
    "\n",
    "*** \n",
    "\n",
    "# Closing thoughts\n",
    "\n",
    "* A Gaussian process (GP) is a non-parametric approach to modeling noisy data\n",
    "\n",
    "* GPs are useful...\n",
    "   * when you don't know the explicit functional form of the correlated noise in your data\n",
    "\n",
    "   * for directly measuring physical parameters in certain circumstances (e.g.: fits for a period in quasi-periodic observations)\n",
    "\n",
    "   * for modeling correlated noise that's present when you know the functional form of your signal (e.g.: )\n",
    "   \n",
    "* GPs are slow in general, so you should take advantage of packages like [`celerite`](http://celerite.readthedocs.io) and [`george`](http://george.readthedocs.io) for fast matrix inversion/decomposition tricks to speed things up\n",
    "\n",
    "* This tutorial only covers GPs in one dimension, though they are extensible to N dimensions (see for example: [GPyTorch](https://github.com/cornellius-gp/gpytorch))\n",
    "\n",
    "* For a the ultimate reference on GPs, we encourage you to read [Rasmussen and Williams (2006)](http://www.gaussianprocess.org/gpml/), which is available for free!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
